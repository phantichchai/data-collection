{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d64d9c-2594-40f1-9e9b-254a8c9a8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c7281d-708d-4e6b-913e-7ab1f91e1a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51bf5f26-03cb-44e4-b664-796448c7e940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phantichchai/anaconda3/envs/try/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import VivitConfig, VivitModel, ViTModel, ViTConfig, BertModel, BertTokenizer, BertConfig, TransfoXLModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6a88c2b-3bf4-4589-865f-a4dbf9c9293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModelEncoder(nn.Module):\n",
    "    def __init__(self, vit_model_name='google/vit-base-patch16-224-in21k',\n",
    "                 vivit_model_name='google/vivit-b-16x2-kinetics400',\n",
    "                 bert_model_name='bert-base-uncased'):\n",
    "        super(MultiModelEncoder, self).__init__()\n",
    "\n",
    "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
    "        self.vivit = VivitModel.from_pretrained(vivit_model_name)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "    def forward(self, image, video, text):\n",
    "        vit_outputs = self.vit(image)\n",
    "        vit_pooled_output = vit_outputs.pooler_output\n",
    "        vit_last_hidden_state = vit_outputs.last_hidden_state\n",
    "\n",
    "        vivit_outputs = self.vivit(video)\n",
    "        vivit_pooled_output = vivit_outputs.pooler_output\n",
    "        vivit_last_hidden_state = vivit_outputs.last_hidden_state\n",
    "\n",
    "        text_encoding = self.bert_tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "        input_ids = text_encoding['input_ids'].to(device)\n",
    "        attention_mask = text_encoding['attention_mask'].to(device)\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_pooled_output = bert_outputs.pooler_output\n",
    "        bert_last_hidden_state = bert_outputs.last_hidden_state\n",
    "\n",
    "        concatenated_output = torch.cat((vit_pooled_output, vivit_pooled_output, bert_pooled_output), dim=1)\n",
    "        last_hidden_states = torch.cat((vit_last_hidden_state, vivit_last_hidden_state, bert_last_hidden_state), dim=1)\n",
    "\n",
    "        return { \n",
    "            \"concatenated_output\": concatenated_output,\n",
    "            \"last_hidden_states\": last_hidden_states\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c318d9a5-e1ba-4e57-ad3d-e94d4df1dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModelDecoder(nn.Module):\n",
    "    def __init__(self, d_model=2304,\n",
    "                 nhead=8,\n",
    "                 num_layers=12\n",
    "                ):\n",
    "        super(MultiModelDecoder, self).__init__()\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_model = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, encoder_last_state, attension_state):\n",
    "        transformer_outputs = self.transformer_model(encoder_last_state, attension_state)\n",
    "        return transformer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eca94fd-ea82-4b6b-a06f-91fee270466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, d_model, hidden_size, num_actions):\n",
    "        super(Policy, self).__init__()\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_size),\n",
    "            nn.Dropout(0.1, False),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Dropout(0.1, False),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.policy(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a39c9c86-b1ca-419c-a5de-0d26b5ce6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Agent, self).__init__()\n",
    "        self.encoder = MultiModelEncoder()\n",
    "        self.decoder = MultiModelDecoder()\n",
    "        self.policy = Policy(2304, 2048, 10)\n",
    "\n",
    "    def forward(self, image, video, text):\n",
    "        encoder_embed = self.encoder(image, video, text)\n",
    "        decoder_embed = self.decoder(encoder_embed['concatenated_output'], encoder_embed['concatenated_output'])\n",
    "        logits = self.policy(decoder_embed)\n",
    "        return logits\n",
    "\n",
    "    def get_model_size(self):\n",
    "        param_size = 0\n",
    "        buffer_size = 0\n",
    "        for param in self.parameters():\n",
    "            param_size += param.numel() * param.element_size()\n",
    "        for buffer in self.buffers():\n",
    "            buffer_size += buffer.numel() * buffer.element_size()\n",
    "\n",
    "        size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "        return { \n",
    "            \"size_all_mb\" : size_all_mb,\n",
    "            \"parameter_size\" : param_size\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d19308-166e-46ca-9281-a4c019d178ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phantichchai/anaconda3/envs/try/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['vivit.pooler.dense.bias', 'vivit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Agent(\n",
       "  (encoder): MultiModelEncoder(\n",
       "    (vit): ViTModel(\n",
       "      (embeddings): ViTEmbeddings(\n",
       "        (patch_embeddings): ViTPatchEmbeddings(\n",
       "          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): ViTEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (pooler): ViTPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (vivit): VivitModel(\n",
       "      (embeddings): VivitEmbeddings(\n",
       "        (patch_embeddings): VivitTubeletEmbeddings(\n",
       "          (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): VivitEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x VivitLayer(\n",
       "            (attention): VivitAttention(\n",
       "              (attention): VivitSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): VivitSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): VivitIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_act_fn): FastGELUActivation()\n",
       "            )\n",
       "            (output): VivitOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (pooler): VivitPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): MultiModelDecoder(\n",
       "    (transformer_model): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2304, out_features=2304, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2304, out_features=2304, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=2304, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=2304, bias=True)\n",
       "          (norm1): LayerNorm((2304,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((2304,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((2304,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (policy): Policy(\n",
       "    (policy): Sequential(\n",
       "      (0): Linear(in_features=2304, out_features=2048, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (4): Dropout(p=0.1, inplace=False)\n",
       "      (5): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (6): Linear(in_features=2048, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent()\n",
    "agent.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08bf2e95-86d1-4826-b998-16359b036f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    image = torch.randn(1, 3, 224, 224).to(device)\n",
    "    video = torch.randn(1, 32, 3, 224, 224).to(device)\n",
    "    text = [\"This is a sample sentence for the text encoder.\"]\n",
    "    logits = agent(image, video, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6e85497-a084-4091-8b0e-56e40d0c7166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21befafa-1921-4465-888e-19e8913aa6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size_all_mb': 3499.4111709594727, 'parameter_size': 3669390376}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_size = agent.get_model_size()\n",
    "model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ece128-1aeb-44e1-9e3c-12135e789b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 3499.41 MB\n",
      "Model parameter: 3669390376\n"
     ]
    }
   ],
   "source": [
    "print(f'Model size: {model_size[\"size_all_mb\"]:.2f} MB')\n",
    "print(f'Model parameter: {model_size[\"parameter_size\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "252dddf1-f9b1-42df-8350-f05b9d3a5f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(agent.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869bba2e-1243-431f-bbe7-4e7e737c3185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
