{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d64d9c-2594-40f1-9e9b-254a8c9a8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c7281d-708d-4e6b-913e-7ab1f91e1a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51bf5f26-03cb-44e4-b664-796448c7e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, ViTConfig, BertModel, BertTokenizer, BertConfig, TransfoXLModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "048f210d-d8ef-49dc-9bf2-5404d551467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CvivitConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=128,\n",
    "        color_channel=3,\n",
    "        num_frames=32,\n",
    "        emb_size=768,\n",
    "        d_model=768,\n",
    "        patch_size=(2, 8, 8),\n",
    "        num_layers_spatial=4,\n",
    "        num_heads_spatial=8,\n",
    "        dim_feedforward_spatial=2048,\n",
    "        dropout_spatial=0.1,\n",
    "        num_layers_temporal=4,\n",
    "        num_heads_temporal=8,\n",
    "        dim_feedforward_temporal=2048,\n",
    "        dropout_temporal=0.1,\n",
    "    ):\n",
    "        self.image_size = image_size\n",
    "        self.color_channel = color_channel\n",
    "        self.num_frames = num_frames\n",
    "        self.emb_size = emb_size\n",
    "        self.d_model = d_model\n",
    "        self.patch_size = patch_size\n",
    "        self.num_layers_spatial = num_layers_spatial\n",
    "        self.num_heads_spatial = num_heads_spatial\n",
    "        self.dim_feedforward_spatial = dim_feedforward_spatial\n",
    "        self.dropout_spatial = dropout_spatial\n",
    "        self.num_layers_temporal = num_layers_temporal\n",
    "        self.num_heads_temporal = num_heads_temporal\n",
    "        self.dim_feedforward_temporal = dim_feedforward_temporal\n",
    "        self.dropout_temporal = dropout_temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ad696aa-e2b1-4b6d-a11a-8411765086d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=768,\n",
    "        hidden_size=2048,\n",
    "        num_actions=10\n",
    "    ):\n",
    "        self.d_model = d_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_actions = num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40565987-9cd4-402d-85c7-32206902b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModelEncoder(nn.Module):\n",
    "    def __init__(self, vit_model_name='google/vit-base-patch16-224-in21k',\n",
    "                 bert_model_name='bert-base-uncased',\n",
    "                 cvivit_config: CvivitConfig=CvivitConfig()):\n",
    "        super(MultiModelEncoder, self).__init__()\n",
    "\n",
    "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
    "        self.cvivit = VideoTransformerModel(\n",
    "            video_dimension=(cvivit_config.color_channel, cvivit_config.image_size, cvivit_config.image_size),\n",
    "            emb_size=cvivit_config.emb_size,\n",
    "            d_model=cvivit_config.d_model,\n",
    "            patch_size=cvivit_config.patch_size,\n",
    "            num_layers_spatial=cvivit_config.num_layers_spatial,\n",
    "            num_heads_spatial=cvivit_config.num_heads_spatial,\n",
    "            dim_feedforward_spatial=cvivit_config.dim_feedforward_spatial,\n",
    "            dropout_spatial=cvivit_config.dropout_spatial,\n",
    "            num_layers_temporal=cvivit_config.num_layers_temporal, \n",
    "            num_heads_temporal=cvivit_config.num_heads_temporal,\n",
    "            dim_feedforward_temporal=cvivit_config.dim_feedforward_temporal,\n",
    "            dropout_temporal=cvivit_config.dropout_temporal\n",
    "        )\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "    def forward(self, image, video, text):\n",
    "        vit_outputs = self.vit(image)\n",
    "        vit_last_hidden_state = vit_outputs.last_hidden_state\n",
    "\n",
    "        cvivit = self.cvivit(video)\n",
    "        \n",
    "        text_encoding = self.bert_tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "        input_ids = text_encoding['input_ids'].to(device)\n",
    "        attention_mask = text_encoding['attention_mask'].to(device)\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        instruction_last_hidden_state = bert_outputs.last_hidden_state\n",
    "\n",
    "        \n",
    "        print(f\"Vit last_hidden_states shape: {vit_last_hidden_state.shape}\")\n",
    "        print(f\"Video last_hidden_states shape: {cvivit.shape}\")\n",
    "        print(f\"Text last_hidden_states shape: {instruction_last_hidden_state.shape}\")\n",
    "        vision_last_hidden_states = torch.cat((vit_last_hidden_state, cvivit), dim=1)\n",
    "        print(f\"Token last_hidden_states shape: {vision_last_hidden_states.shape}\")\n",
    "        return {\n",
    "            \"instruction_last_hidden_state\": instruction_last_hidden_state,\n",
    "            \"vision_last_hidden_states\": vision_last_hidden_states\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22b0c6db-8092-4f63-89d5-5d92cde524f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModelDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=768,\n",
    "        nhead=8,\n",
    "        num_layers=4\n",
    "    ):\n",
    "        super(MultiModelDecoder, self).__init__()\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=self.decoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, instruction, memory):\n",
    "        x = self.decoder(instruction, memory)\n",
    "        print(f\"Decoder outputs shape: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eca94fd-ea82-4b6b-a06f-91fee270466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, policy_config=PolicyConfig()):\n",
    "        super(Policy, self).__init__()\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(policy_config.d_model, policy_config.hidden_size),\n",
    "            nn.Dropout(0.1, False),\n",
    "            nn.LayerNorm(policy_config.hidden_size),\n",
    "            nn.Linear(policy_config.hidden_size, policy_config.hidden_size),\n",
    "            nn.Dropout(0.1, False),\n",
    "            nn.LayerNorm(policy_config.hidden_size),\n",
    "            nn.Linear(policy_config.hidden_size, policy_config.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.policy(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a39c9c86-b1ca-419c-a5de-0d26b5ce6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Agent, self).__init__()\n",
    "        self.encoder = MultiModelEncoder()\n",
    "        self.decoder = MultiModelDecoder()\n",
    "        self.policy = Policy()\n",
    "\n",
    "    def forward(self, image, video, text):\n",
    "        encoder_outputs = self.encoder(image, video, text)\n",
    "        decoder_outputs = self.decoder(encoder_outputs['instruction_last_hidden_state'], encoder_outputs['vision_last_hidden_states'])\n",
    "        logits = self.policy(decoder_outputs)\n",
    "        return logits\n",
    "\n",
    "    def get_model_size(self):\n",
    "        param_size = 0\n",
    "        buffer_size = 0\n",
    "        for param in self.parameters():\n",
    "            param_size += param.numel() * param.element_size()\n",
    "        for buffer in self.buffers():\n",
    "            buffer_size += buffer.numel() * buffer.element_size()\n",
    "\n",
    "        size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "        return { \n",
    "            \"size_all_mb\" : size_all_mb,\n",
    "            \"parameter_size\" : param_size\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08bf2e95-86d1-4826-b998-16359b036f95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerEncoder3D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 2\u001b[0m     agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     agent\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m     image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Agent, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mMultiModelEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m MultiModelDecoder()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m Policy()\n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mMultiModelEncoder.__init__\u001b[0;34m(self, vit_model_name, bert_model_name, cvivit_config)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28msuper\u001b[39m(MultiModelEncoder, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit \u001b[38;5;241m=\u001b[39m ViTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(vit_model_name)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcvivit \u001b[38;5;241m=\u001b[39m \u001b[43mVideoTransformerModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_dimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43memb_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers_spatial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers_spatial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads_spatial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads_spatial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_feedforward_spatial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_feedforward_spatial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_spatial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_spatial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers_temporal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers_temporal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads_temporal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads_temporal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_feedforward_temporal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_feedforward_temporal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_temporal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvivit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_temporal\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(bert_model_name)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(bert_model_name)\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mVideoTransformerModel.__init__\u001b[0;34m(self, video_dimension, emb_size, d_model, patch_size, num_layers_spatial, num_heads_spatial, dim_feedforward_spatial, dropout_spatial, num_layers_temporal, num_heads_temporal, dim_feedforward_temporal, dropout_temporal)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28msuper\u001b[39m(VideoTransformerModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embedding \u001b[38;5;241m=\u001b[39m PatchEmbedding(video_dimension, patch_size, emb_size)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerEncoder3D\u001b[49m(d_model, num_layers_spatial, num_heads_spatial, dim_feedforward_spatial, dropout_spatial)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_transformer \u001b[38;5;241m=\u001b[39m TemporalTransformerEncoder(d_model, num_layers_temporal, num_heads_temporal, dim_feedforward_temporal, dropout_temporal)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TransformerEncoder3D' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    agent = Agent()\n",
    "    agent.to(device)\n",
    "    image = torch.randn(2, 3, 224, 224).to(device)\n",
    "    video = torch.randn(2, 32, 3, 128, 128).to(device)\n",
    "    text = [\"This is a sample sentence for the text encoder.\", \"This is a sample sentence for the text encoder.\"]\n",
    "    logits = agent(image, video, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6e85497-a084-4091-8b0e-56e40d0c7166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2085, -1.3850,  0.0412,  0.0765,  0.3234, -0.7414,  0.7565,\n",
       "            1.0119,  0.5515,  0.8427],\n",
       "          [ 0.2577, -1.6940,  0.4986,  0.5504,  0.5008, -0.8456,  0.0920,\n",
       "            0.9436,  0.9753,  1.0063],\n",
       "          [-0.7221, -0.8581,  0.4347,  0.0464,  0.5096, -0.5257,  0.0034,\n",
       "            0.2765,  1.2155,  0.5176],\n",
       "          [ 0.3543, -1.2693,  0.1108,  0.8030,  1.0233, -0.6481,  0.8123,\n",
       "            0.5214,  0.1852,  1.5182],\n",
       "          [ 0.1190, -1.3624,  0.0703,  0.5090,  0.5268, -1.0735,  0.7735,\n",
       "           -0.0508,  0.5317,  1.0983],\n",
       "          [ 0.3523, -0.7707,  0.0971,  0.3184,  0.8897, -0.7063,  0.0821,\n",
       "            0.4055,  0.4633,  0.8298],\n",
       "          [ 0.2188, -1.1613,  1.3514,  0.0692,  0.0376, -1.0174,  0.1084,\n",
       "            0.6880,  1.5006,  1.2779],\n",
       "          [-0.6426, -1.0456,  0.6534,  0.6634,  0.8785, -0.4172,  0.5121,\n",
       "            0.1043,  0.7147,  0.7445],\n",
       "          [ 0.0431, -1.9833,  0.2333,  0.0348,  0.8297, -0.8826,  0.4583,\n",
       "            0.3087,  0.2133,  1.2743],\n",
       "          [-0.4386, -0.5741,  0.4612,  0.1699,  1.4142, -0.4386, -0.1841,\n",
       "            0.8712,  0.6587,  0.7829],\n",
       "          [-0.3907, -0.8628,  0.5162, -0.2121,  1.1348, -0.4415,  0.0792,\n",
       "            0.5760,  0.6226,  0.7800],\n",
       "          [-0.2757, -0.8874,  0.7671,  0.4425,  1.0787, -0.5712,  0.4801,\n",
       "           -0.1374,  0.6276,  0.7643],\n",
       "          [-0.0893, -1.6321,  0.4464,  0.2141,  1.0382, -0.9915,  0.5092,\n",
       "            0.0042,  0.6367,  0.7088],\n",
       "          [-0.2068, -1.1006,  0.2978, -0.1336,  0.5402, -0.8063,  0.0643,\n",
       "           -0.0908,  0.5667,  0.3496]],\n",
       " \n",
       "         [[-0.0961, -1.5480,  0.6941,  0.2966,  0.7327, -1.0964,  0.8905,\n",
       "           -0.0826,  0.0205,  0.9721],\n",
       "          [ 0.1076, -0.6132,  0.7245,  0.3070,  0.6739, -0.7573, -0.6870,\n",
       "            0.0120,  1.0065,  0.8036],\n",
       "          [-0.0692, -1.3730,  1.0957,  0.5354,  1.1509, -0.4237, -0.2502,\n",
       "            0.2362,  0.6493,  0.6583],\n",
       "          [-0.2025, -0.9344,  0.5925,  0.6987,  1.0679, -0.7103,  0.0574,\n",
       "            0.6967,  0.6237,  0.9048],\n",
       "          [-0.0992, -0.8154,  1.0984,  0.3246,  1.3346, -0.4788,  0.6511,\n",
       "            0.4940,  0.3714,  0.7578],\n",
       "          [-0.1556, -1.0099,  0.1162,  0.6011,  1.1018, -0.9314,  0.0530,\n",
       "            0.4274,  0.4687,  0.8183],\n",
       "          [-0.1905, -1.4115,  1.0878,  0.9327,  0.7944, -0.8978,  0.1112,\n",
       "            0.7403,  0.9535,  1.3264],\n",
       "          [-0.4920, -0.9886,  0.8017,  0.5241,  1.1311, -0.9999,  0.0656,\n",
       "            0.6396,  0.6758,  0.5558],\n",
       "          [-0.1308, -0.8644,  0.6773,  0.2953,  1.0657, -0.8547,  0.3315,\n",
       "            0.2833,  0.3432,  0.8381],\n",
       "          [ 0.0329, -0.8446,  0.2668,  0.5757,  0.9731, -1.1356, -0.0568,\n",
       "            0.8757,  0.5507,  0.7219],\n",
       "          [-0.3977, -1.0271,  0.9039,  0.4168,  1.1664, -0.6817, -0.1393,\n",
       "            0.6873,  0.3631,  1.1441],\n",
       "          [-0.4521, -0.7168,  0.8861,  0.3654,  0.4948, -0.9302,  0.1376,\n",
       "            0.1351,  0.6275,  1.5720],\n",
       "          [ 0.2644, -1.1378,  0.8328, -0.0130,  0.7557, -0.6791, -0.0127,\n",
       "            0.8322,  0.4362,  1.0073],\n",
       "          [-0.0111, -0.5631,  0.5904, -0.1563,  0.7973, -0.5905,  0.3325,\n",
       "            0.1947,  1.0614,  0.5085]]], device='cuda:0'),\n",
       " torch.Size([2, 14, 10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21befafa-1921-4465-888e-19e8913aa6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size_all_mb': 1098.5605850219727, 'parameter_size': 1151916072}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_size = agent.get_model_size()\n",
    "model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20ece128-1aeb-44e1-9e3c-12135e789b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1098.56 MB\n",
      "Model parameter: 1151916072\n"
     ]
    }
   ],
   "source": [
    "print(f'Model size: {model_size[\"size_all_mb\"]:.2f} MB')\n",
    "print(f'Model parameter: {model_size[\"parameter_size\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53bf3a-bf8f-48ba-a1f1-180809a63668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
